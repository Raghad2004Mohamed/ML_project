# -*- coding: utf-8 -*-
"""ML project script-Copy1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Qbwgz0V4VT6Y_GPjtR8BiHemDuramB0g
"""

## Project Research Question:
#----------------------------------------------
##**Can we build a machine learning model to predict whether a patient is experiencing an epileptic seizure based on EEG signals?**##

# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.metrics import (accuracy_score, classification_report,
                           confusion_matrix, roc_curve, auc,
                           precision_recall_curve, average_precision_score)
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# Load dataset
df = pd.read_csv('Epileptic Seizure Recognition.csv')

print(df.info())

# Check first few rows
df.head()

#Drop the ID column
df = df.drop(['Unnamed'], axis=1)

# Check missing values
print(df.isnull().sum())

# Describe numerical features
print(df.describe())

#Handle missing values (replace NaN with column mean)
df.fillna(df.mean(), inplace=True)

# Verify missing values are handled
print(df.isnull().sum())

# Assuming EEG data is in columns like 'X1', 'X2', ..., 'X178'
signal_cols = [col for col in df.columns if col.startswith('X')]

# Create new feature: mean EEG signal
df['mean_signal'] = df[signal_cols].mean(axis=1)

# histogram
plt.figure(figsize=(8,6))
sns.histplot(df[df.columns[30:50]], kde=True, bins=30)
plt.title(f'Histogram of {df.columns[0]}')
plt.show()

# heatmap
plt.figure(figsize=(100,50))
corr = df.corr()
sns.heatmap(corr, annot=True, fmt=".2f", cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

#box plot
plt.figure(figsize=(10, 6))
sns.boxplot(data=df.iloc[:, 1:50])
plt.title("EEG Signal Distributions")

#pca plot
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(df.drop('y', axis=1))
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=df['y'])

#signals plot
plt.figure(figsize=(12, 6))
plt.plot(df.iloc[0, 1:50])
plt.title("Sample EEG Signal")

# Target Variable Analysis (assuming 'y' is target)
print("\nTarget variable distribution:")
print(df['y'].value_counts(normalize=True))

plt.figure(figsize=(8,6))
sns.countplot(x='y', data=df)
plt.title("Distribution of Target Variable")
plt.show()

#Correlation Analysis
plt.figure(figsize=(12,8))
corr = df.corr()
sns.heatmap(corr, cmap='coolwarm', center=0)
plt.title("Feature Correlation Heatmap")
plt.show()

#Feature Distributions (univariate)
num_cols = df.select_dtypes(include=['int64','float64']).columns
for col in num_cols[:5]:  # First 5 numerical features
    plt.figure(figsize=(8,4))
    sns.histplot(df[col], kde=True)
    plt.title(f"Distribution of {col}")
    plt.show()

#Feature vs Target Analysis (bivariate)
for col in num_cols[:3]:  # First 3 features vs target
    plt.figure(figsize=(8,4))
    sns.boxplot(x='y', y=col, data=df)
    plt.title(f"{col} by Target Class")
    plt.show()

#Outlier Detection
for col in num_cols[:3]:
    plt.figure(figsize=(8,4))
    sns.boxplot(x=df[col])
    plt.title(f"Boxplot of {col}")
    plt.show()

#Time Series Visualization (for EEG data)
plt.figure(figsize=(12,6))
sample_idx = 0  # First sample
plt.plot(df.iloc[sample_idx, :-1])  # All features except target
plt.title(f"EEG Signal Sample (Class: {df.iloc[sample_idx, -1]})")
plt.xlabel("Time Points")
plt.ylabel("Signal Value")
plt.show()

# Assuming y=1 is seizure and others are non-seizure
df['binary_label'] = df['y'].apply(lambda x: 1 if x == 1 else 0)

#Separate features and target
X = df.drop('y', axis=1)
y = df['y']

# Remove constant features
X_non_constant = X.loc[:, (X != X.iloc[0]).any(axis=0)]

#Scale the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Feature selection: Remove features with very low variance
from sklearn.feature_selection import VarianceThreshold
variance_threshold = 0.01
selector_variance = VarianceThreshold(threshold=variance_threshold)
X_non_constant = selector_variance.fit_transform(X_scaled)

# Check the feature variances after variance thresholding
variances = np.var(X_non_constant, axis=0)
print(f"Min Variance: {np.min(variances)} | Max Variance: {np.max(variances)} | Mean Variance: {np.mean(variances)}")

# Scale the features after removing constant and low-variance features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_non_constant)

# Feature selection: select top 20 features based on ANOVA F-test
selector_kbest = SelectKBest(score_func=f_classif, k=20)
X_selected = selector_kbest.fit_transform(X_non_constant, y)

#Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)

print(f"Shape of X_train: {X_train.shape} | Shape of X_test: {X_test.shape}")

# New feature: Mean EEG signal per sample
df['EEG_mean'] = df.iloc[:, 1:179].mean(axis=1)

correlations = df.corr()['y'].abs().sort_values(ascending=False)
top_features = correlations[1:11].index  # Top 10 features
X = df[top_features]
y = df['y']

#Initialize models
models = {
    'SVM': SVC(random_state=42),
    'DecisionTree': DecisionTreeClassifier(random_state=42),
    'KNN': KNeighborsClassifier(),
    'RandomForest': RandomForestClassifier(random_state=42),
    'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42)
}

from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.preprocessing import label_binarize
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import roc_curve, auc

# Define models dictionary with models (ensure to set probability=True for SVC)
models = {
    'SVC': SVC(probability=True),

}

# Train + Evaluate
for name, model in models.items():
    # Fit the model on the training data
    model.fit(X_train, y_train)

    # Make predictions
    y_pred = model.predict(X_test)

    # Check if the model supports predict_proba
    if hasattr(model, 'predict_proba'):
        y_proba = model.predict_proba(X_test)
    else:
        y_proba = None

    # Metrics
    print(f"\n{name}")
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
    print("Classification Report:\n", classification_report(y_test, y_pred))
    print("Accuracy:", accuracy_score(y_test, y_pred))

    # Plot Confusion Matrix
    plt.figure(figsize=(5,4))
    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix - {name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

    # If the model supports `predict_proba`, plot ROC Curve
    if y_proba is not None:
        y_test_bin = label_binarize(y_test, classes=np.unique(y_test))  # Binarize the output labels

        # For each class, plot the ROC curve
        plt.figure(figsize=(6,5))
        for i in range(y_test_bin.shape[1]):
            fpr, tpr, thresholds = roc_curve(y_test_bin[:, i], y_proba[:, i])
            roc_auc = auc(fpr, tpr)
            plt.plot(fpr, tpr, label=f'Class {i} ROC Curve (AUC = {roc_auc:.2f})')

        plt.plot([0,1], [0,1], linestyle='--', color='gray')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title(f'ROC Curve - {name}')
        plt.legend()
        plt.show()

#GridSearchCV for SVM
param_grid_svm = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf'],
    'gamma': ['scale', 'auto']
}

grid_search_svm = GridSearchCV(SVC(probability=True, random_state=42), param_grid_svm, cv=5, scoring='accuracy')
grid_search_svm.fit(X_train, y_train)

print("Best SVM Parameters")
print(grid_search_svm.best_params_)

#Evaluate tuned SVM
best_svm = grid_search_svm.best_estimator_
best_pred = best_svm.predict(X_test)
best_proba = best_svm.predict_proba(X_test)[:,1]

print("\n=== Tuned SVM Results ===")
print("Confusion Matrix:\n", confusion_matrix(y_test, best_pred))
print("Classification Report:\n", classification_report(y_test, best_pred))
print("Accuracy:", accuracy_score(y_test, best_pred))

#Plot Confusion Matrix for tuned SVM
plt.figure(figsize=(5,4))
sns.heatmap(confusion_matrix(y_test, best_pred), annot=True, fmt='d', cmap='Greens')
plt.title('Confusion Matrix - Tuned SVM')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

#SVM model after hyperparameter tuning
svm_model = SVC(probability=True)

# Train your model
svm_model.fit(X_train, y_train)

# Get probabilities for each class
best_proba = svm_model.predict_proba(X_test)

# Check the shape of best_proba
print(best_proba.shape)

#proceed to plot the ROC curve
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Binarize the output labels (for multiclass ROC)
y_test_bin = label_binarize(y_test, classes=np.unique(y_test))

# Plot ROC curve for each class
plt.figure(figsize=(6, 5))
for i in range(y_test_bin.shape[1]):
    fpr_best, tpr_best, _ = roc_curve(y_test_bin[:, i], best_proba[:, i])
    roc_auc_best = auc(fpr_best, tpr_best)
    plt.plot(fpr_best, tpr_best, label=f'Class {i} ROC Curve (AUC = {roc_auc_best:.2f})')

# Plot the diagonal line for random chance
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Tuned SVM')
plt.legend()
plt.show()

#logestic regression
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

# Initialize
log_reg = LogisticRegression(max_iter=1000, random_state=42)

# Hyperparameter tuning
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10],
    'penalty': ['l1', 'l2'],
    'solver': ['liblinear']
}
grid_log = GridSearchCV(log_reg, param_grid, cv=5, scoring='f1')
grid_log.fit(X_train, y_train)

# Best model
best_log = grid_log.best_estimator_

#random forest
from sklearn.ensemble import RandomForestClassifier

# Initialize
rf = RandomForestClassifier(random_state=42)

# Hyperparameter tuning
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5]
}
grid_rf = GridSearchCV(rf, param_grid, cv=5, scoring='f1')
grid_rf.fit(X_train, y_train)

# Best model
best_rf = grid_rf.best_estimator_

#SVC
from sklearn.svm import SVC

# Initialize
svm = SVC(probability=True, random_state=42)

# Hyperparameter tuning
param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf'],
    'gamma': ['scale', 'auto']
}
grid_svm = GridSearchCV(svm, param_grid, cv=3, scoring='f1')  # Smaller cv due to SVM's slowness
grid_svm.fit(X_train, y_train)

# Best model
best_svm = grid_svm.best_estimator_

from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

def evaluate_model(model, X_test, y_test, name):
    y_pred = model.predict(X_test)

    print(f"\n=== {name} ===")
    print("Best Params:", grid_search.best_params_ if 'grid_' in name.lower() else "N/A")
    print(classification_report(y_test, y_pred))

    # ROC-AUC calculation for multi-class
    if hasattr(model, "predict_proba"):
        y_proba = model.predict_proba(X_test)
        # For multi-class, we need to specify the multi_class parameter
        try:
            roc_auc = roc_auc_score(y_test, y_proba, multi_class='ovr')
            print(f"ROC-AUC (OVR): {roc_auc:.3f}")
        except ValueError:
            # If binary classification or other error
            print("Could not calculate ROC-AUC with OVR method")
    else:
        print("Model doesn't support predict_proba, skipping ROC-AUC")

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f"{name} Confusion Matrix")
    plt.xlabel('Predicted labels')
    plt.ylabel('True labels')
    plt.show()

# Evaluate all models
evaluate_model(best_log, X_test, y_test, "Logistic Regression (Tuned)")
evaluate_model(best_rf, X_test, y_test, "Random Forest (Tuned)")
evaluate_model(best_svm, X_test, y_test, "SVM (Tuned)")

from sklearn.ensemble import (
    RandomForestClassifier, AdaBoostClassifier, VotingClassifier
)
from sklearn.metrics import classification_report, confusion_matrix

from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier

models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'K-Nearest Neighbors': KNeighborsClassifier(),
    'Naive Bayes': GaussianNB()
}

results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    results[name] = accuracy_score(y_test, y_pred)

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
results['Random Forest'] = accuracy_score(y_test, rf.predict(X_test))

ab = AdaBoostClassifier(n_estimators=50, random_state=42)
ab.fit(X_train, y_train)
results['AdaBoost'] = accuracy_score(y_test, ab.predict(X_test))

voting = VotingClassifier(estimators=[
    ('lr', LogisticRegression(max_iter=1000)),
    ('knn', KNeighborsClassifier()),
    ('nb', GaussianNB())
], voting='hard')

voting.fit(X_train, y_train)
results['Voting Ensemble'] = accuracy_score(y_test, voting.predict(X_test))

results = {}
reports = {}
conf_matrices = {}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Accuracy
    acc = accuracy_score(y_test, y_pred)
    results[name] = acc

    # Classification report (dictionary format for later use)
    reports[name] = classification_report(y_test, y_pred, output_dict=True)

    # Confusion matrix
    conf_matrices[name] = confusion_matrix(y_test, y_pred)

# Summary table with Accuracy
results_df = pd.DataFrame(list(results.items()), columns=['Model', 'Accuracy'])
print("\n Accuracy Summary:")
print(results_df)

# Detailed classification reports
for name, report in reports.items():
    print(f"\nClassification Report for {name}:\n")
    print(classification_report(y_test, models[name].predict(X_test)))

# Confusion matrices
for name, matrix in conf_matrices.items():
    plt.figure(figsize=(4, 3))
    sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix: {name}')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.tight_layout()
    plt.show()

from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

# Define models
lr = LogisticRegression(max_iter=1000)
nb = GaussianNB()
knn = KNeighborsClassifier()

# Hard Voting
voting_hard = VotingClassifier(
    estimators=[('lr', lr), ('nb', nb), ('knn', knn)],
    voting='hard'
)

# Soft Voting
voting_soft = VotingClassifier(
    estimators=[('lr', lr), ('nb', nb), ('knn', knn)],
    voting='soft'
)

#Train & Evaluate Hard Voting
voting_hard.fit(X_train, y_train)
y_pred_hard = voting_hard.predict(X_test)
print(" Hard Voting Accuracy:", accuracy_score(y_test, y_pred_hard))
print(" Classification Report (Hard Voting):")
print(classification_report(y_test, y_pred_hard))

plt.figure(figsize=(4, 3))
sns.heatmap(confusion_matrix(y_test, y_pred_hard), annot=True, fmt='d', cmap='Blues')
plt.title(' Confusion Matrix - Hard Voting')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.tight_layout()
plt.show()

#Train & Evaluate Soft Voting
voting_soft.fit(X_train, y_train)
y_pred_soft = voting_soft.predict(X_test)
print("\n Soft Voting Accuracy:", accuracy_score(y_test, y_pred_soft))
print(" Classification Report (Soft Voting):")
print(classification_report(y_test, y_pred_soft))

plt.figure(figsize=(4, 3))
sns.heatmap(confusion_matrix(y_test, y_pred_soft), annot=True, fmt='d', cmap='Oranges')
plt.title(' Confusion Matrix - Soft Voting')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.tight_layout()
plt.show()

from sklearn.ensemble import BaggingClassifier
from sklearn.linear_model import LogisticRegression

bagging_model = BaggingClassifier(
    estimator=LogisticRegression(max_iter=1000),
    n_estimators=10,
    random_state=42
)
bagging_model.fit(X_train, y_train)
print("Bagging Accuracy:", bagging_model.score(X_test, y_test))

from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

boosting_model = AdaBoostClassifier(
    estimator=DecisionTreeClassifier(max_depth=2),
    n_estimators=50,
    learning_rate=1.0,
    random_state=42
)
boosting_model.fit(X_train, y_train)
print("Boosting Accuracy:", boosting_model.score(X_test, y_test))

from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier

# Define base learners
base_models = [
    ('lr', LogisticRegression(max_iter=1000)),
    ('knn', KNeighborsClassifier()),
    ('nb', GaussianNB())
]

# Meta-learner (final decision)
stacking_model = StackingClassifier(
    estimators=base_models,
    final_estimator=LogisticRegression(),
    cv=5
)

stacking_model.fit(X_train, y_train)
print("Stacking Accuracy:", stacking_model.score(X_test, y_test))

# Define models
models = {
    "Bagging (LogReg)": BaggingClassifier(estimator=LogisticRegression(max_iter=1000), n_estimators=10, random_state=42),
    "Boosting (Tree)": AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=2), n_estimators=50, random_state=42),
    "Stacking (LR+KNN+NB)": StackingClassifier(
        estimators=[
            ('lr', LogisticRegression(max_iter=1000)),
            ('knn', KNeighborsClassifier()),
            ('nb', GaussianNB())
        ],
        final_estimator=LogisticRegression(),
        cv=5
    ),
    "Voting (LR+KNN+NB)": VotingClassifier(
        estimators=[
            ('lr', LogisticRegression(max_iter=1000)),
            ('knn', KNeighborsClassifier()),
            ('nb', GaussianNB())
        ],
        voting='hard'
    )
}

# Collect metrics
summary = []
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    report = classification_report(y_test, y_pred, output_dict=True)
    summary.append({
        "Model": name,
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": report['weighted avg']['precision'],
        "Recall": report['weighted avg']['recall'],
        "F1-score": report['weighted avg']['f1-score']
    })

df_summary = pd.DataFrame(summary)
print(df_summary)

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Your full model comparison data
df = pd.DataFrame({
    "Model": ["Bagging (LogReg)", "Boosting (Tree)", "Stacking (LR+KNN+NB)", "Voting (LR+KNN+NB)"],
    "Accuracy": [0.408696, 0.490870, 0.530870, 0.463913],
    "Precision": [0.405603, 0.495849, 0.530129, 0.475394],
    "Recall": [0.408696, 0.490870, 0.530870, 0.463913],
    "F1-score": [0.405395, 0.476045, 0.529224, 0.455507],
})

# Reshape the DataFrame for seaborn
df_melted = df.melt(id_vars="Model", var_name="Metric", value_name="Score")

# Plot everything
plt.figure(figsize=(10, 6))
sns.barplot(data=df_melted, x="Model", y="Score", hue="Metric", palette="Set2")
plt.title("Performance Comparison of Ensemble Models")
plt.xticks(rotation=15)
plt.ylim(0, 1)
plt.legend(title="Metric")
plt.tight_layout()
plt.show()

# If X_train is a numpy array, convert it to a pandas DataFrame
X_train_df = pd.DataFrame(X_train)
X_train_df.columns = [f'Feature_{i}' for i in range(X_train_df.shape[1])]

# Feature Importance - Random Forest
rf_importances = pd.DataFrame({
    'Feature': X_train_df.columns,
    'Importance': best_rf.feature_importances_
}).sort_values(by='Importance', ascending=False)

# Display top 20 features if there are many
top_n = min(20, len(rf_importances))
plt.figure(figsize=(10, 8))
sns.barplot(data=rf_importances.head(top_n), x='Importance', y='Feature', palette='Blues_r')
plt.title('Top Feature Importance - Random Forest')
plt.tight_layout()
plt.show()